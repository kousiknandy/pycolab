{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNj1Rn6U5Elk4h0jCAKIY9j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kousiknandy/pycolab/blob/main/Crawler_Hostcounter_Asyncio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "s-y8AwFRcLci"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib3.util import parse_url\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import random\n",
        "from functools import partial, reduce\n",
        "from itertools import tee\n",
        "from collections import Counter, defaultdict\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import heapq\n",
        "\n",
        "base_host = \"simple.m.wikipedia.org\"\n",
        "max_depth = 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def scrape_page(url):\n",
        "    await asyncio.sleep(1+ random.random())\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        async with session.get(url) as resp:\n",
        "            data = await resp.text()\n",
        "    soup = BeautifulSoup(data, \"html.parser\")\n",
        "    links = soup.find_all(\"a\")\n",
        "    hosts, urls = tee(l.get(\"href\") for l in links if l.get(\"href\"))\n",
        "    hosts = (parse_url(h).hostname or base_host for h in hosts)\n",
        "    urls = (\n",
        "        l\n",
        "        for l in urls\n",
        "        if parse_url(l).hostname == None and parse_url(l).path.startswith(\"/wiki/\")\n",
        "    )\n",
        "    return hosts, urls\n",
        "\n"
      ],
      "metadata": {
        "id": "FMjxyW5CdAxV"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def mapper(url, depth):\n",
        "    hosts, urls = await scrape_page(\"https://\" + base_host + url)\n",
        "    hosts = Counter(hosts)\n",
        "    return depth, hosts, urls"
      ],
      "metadata": {
        "id": "zPciswtrz4dV"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def host_hash(host, parts):\n",
        "    h = 5381\n",
        "    for c in host:\n",
        "        h = h * 33 + ord(c)\n",
        "    return h % parts\n",
        "\n",
        "def partitioner(parts):\n",
        "    hashes = [defaultdict(list) for _ in range(parts)]\n",
        "    while hc := (yield):\n",
        "        hashes[host_hash(hc[0], parts)][hc[0]].append(hc[1])\n",
        "    return hashes"
      ],
      "metadata": {
        "id": "GO8oMw6K1uds"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def host_summer(hashes):\n",
        "    host_count = {}\n",
        "    for h, l in hashes.items():\n",
        "        host_count[h] = sum(l)\n",
        "    return host_count\n",
        ""
      ],
      "metadata": {
        "id": "LT5dXKeB37eU"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def main(seed_url, partitions):\n",
        "    p = partitioner(partitions)\n",
        "    p.send(None)\n",
        "    pending = []\n",
        "    visited_url = set()\n",
        "    to_scrape = [(seed_url, 0)]\n",
        "    while to_scrape or pending:\n",
        "        # print(to_scrape)\n",
        "        ts = [asyncio.create_task(mapper(u, d)) for u,d in to_scrape]\n",
        "        to_scrape = []\n",
        "        done, pending = await asyncio.wait(ts + list(pending), return_when=asyncio.FIRST_COMPLETED)\n",
        "        for t in done:\n",
        "            d, hosts, urls = await t\n",
        "            for h, c in hosts.items():\n",
        "                p.send((h,c))\n",
        "            if d < max_depth and len(visited_url) < 10:\n",
        "                urls = [u for u in urls if u not in visited_url]\n",
        "                visited_url.update(urls)\n",
        "                to_scrape += [(u, d+1) for u in urls]\n",
        "    try:\n",
        "        p.send(None)\n",
        "    except StopIteration as e:\n",
        "        partitions = e.value\n",
        "    with ProcessPoolExecutor(max_workers=2) as executor:\n",
        "        res = executor.map(host_summer, partitions)\n",
        "    h_c = reduce(lambda a, b: {**a, **b}, res )\n",
        "    return h_c"
      ],
      "metadata": {
        "id": "SNcriPXE4dIT"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hosts = await main(\"/wiki/Computer_science\", 2)\n",
        "hosts = [(v, k) for k,v in hosts.items()]\n",
        "heapq.nlargest(15, hosts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgE08Y-LDs5w",
        "outputId": "5151f9d5-848f-4b0f-95a4-128c4a09a5e1"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(10611, 'simple.m.wikipedia.org'),\n",
              " (319, 'foundation.wikimedia.org'),\n",
              " (268, 'simple.wikipedia.org'),\n",
              " (149, 'web.archive.org'),\n",
              " (135, 'archive.org'),\n",
              " (123, 'en.wikipedia.org'),\n",
              " (109, 'www.mediawiki.org'),\n",
              " (106, 'es.wikipedia.org'),\n",
              " (105, 'www.wikimedia.org'),\n",
              " (105, 'stats.wikimedia.org'),\n",
              " (105, 'foundation.m.wikimedia.org'),\n",
              " (105, 'donate.wikimedia.org'),\n",
              " (105, 'developer.wikimedia.org'),\n",
              " (102, 'creativecommons.org'),\n",
              " (101, 'uk.wikipedia.org')]"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    }
  ]
}