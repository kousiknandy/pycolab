{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNX3aaTnEDiTIqVkXGwjQbx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kousiknandy/pycolab/blob/main/Wordcount_Asyncio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "FsUOlyes4Mb9"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import aiohttp\n",
        "import random\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "url_prefix = \"https://raw.githubusercontent.com/kousiknandy/pycolab/refs/heads/main/data/book/frankenstein/xba\"\n",
        "wordlen = 8\n",
        "\n",
        "async def fetch_text(suffx):\n",
        "    await asyncio.sleep(random.random()+0.5)\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        async with session.get(url_prefix + suffx) as resp:\n",
        "            async for line in resp.content:\n",
        "                yield line\n",
        "\n",
        "async def fetch_words(suffx):\n",
        "    async for line in  fetch_text(suffx):\n",
        "        words = re.finditer(r\"\\b\\w+\\b\", line.decode(\"UTF-8\"), re.ASCII)\n",
        "        for wm in words:\n",
        "            w = wm.group(0).lower()\n",
        "            if len(w) >= wordlen:\n",
        "                yield w\n",
        "\n",
        "async def mapper(suffx):\n",
        "    words = defaultdict(int)\n",
        "    async for w in fetch_words(suffx):\n",
        "        words[w] += 1\n",
        "    return words\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def wordhash(word, m):\n",
        "    h = 5381\n",
        "    for c in word:\n",
        "        h = h * 33 + ord(c)\n",
        "    return h % m\n",
        "\n",
        "partitions = 2\n",
        "bucket = lambda word: wordhash(word, partitions)\n",
        "\n",
        "def partition(words, parts):\n",
        "    for w,c in words.items():\n",
        "        parts[bucket(w)][w].append(c)\n"
      ],
      "metadata": {
        "id": "VLaxt6zk7Oza"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "from functools import partial\n",
        "\n",
        "def top_few(word, n):\n",
        "    counts = [(sum(v), k) for k,v in word.items()]\n",
        "    tops = heapq.nlargest(n, counts)\n",
        "    return tops"
      ],
      "metadata": {
        "id": "YZdBiPth8rM3"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "async def main(file_suffixes):\n",
        "    fs = [asyncio.create_task(mapper(f)) for f in file_suffixes]\n",
        "    parts = [defaultdict(list) for _ in range(partitions)]\n",
        "    for r in asyncio.as_completed(fs):\n",
        "        words = await r\n",
        "        partition(words, parts)\n",
        "    reducers = [partial(top_few, word=parts[i], n=10) for i in range(partitions)]\n",
        "    loop = asyncio.get_event_loop()\n",
        "    tops = []\n",
        "    with ProcessPoolExecutor(max_workers=2) as pool:\n",
        "        fs = [loop.run_in_executor(pool, r) for r in reducers]\n",
        "        for r in asyncio.as_completed(fs):\n",
        "            t = await r\n",
        "            tops.append(t)\n",
        "    res = heapq.merge(*tops)\n",
        "    return res"
      ],
      "metadata": {
        "id": "sx34Xf8B9oYH"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from string import ascii_lowercase\n",
        "file_suffixes = random.sample(ascii_lowercase[:25], k=6)\n",
        "\n",
        "res = await main(file_suffixes=file_suffixes)\n",
        "print(list(res)[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryQtSSGyABdx",
        "outputId": "adaa9cad-5b42-4ba7-a2d4-aacb73ecf9a0"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(18, 'feelings'), (12, 'magistrate'), (12, 'countenance'), (11, 'expressed'), (11, 'creature'), (10, 'darkness'), (10, 'continued'), (9, 'evidence'), (8, 'witnesses'), (8, 'returned')]\n"
          ]
        }
      ]
    }
  ]
}